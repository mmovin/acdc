{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import isnan\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from scipy.stats import bootstrap\n",
    "from carla.data.catalog import OnlineCatalog"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "dataset_name = 'spotify_classic'\n",
    "model_type = \"ann\"\n",
    "model_type_dict = \"ann\"\n",
    "data = OnlineCatalog(data_name=dataset_name)\n",
    "num_features = len(data.continuous)\n",
    "#result_path = 'results_remote/results'\n",
    "result_path = os.path.join(\"~\", \"carla\", \"results\")\n",
    "remove_acdc = False\n",
    "remove_vaes = False\n",
    "vaes = ['crud','revise', 'clue']\n",
    "sess_results_file = False\n",
    "suffix = ''\n",
    "\n",
    "\n",
    "\n",
    "rms = \"dice_cchvae_cruds_proto_roar_wachter_gs_acdc_retrained\"\n",
    "rms_array = [\"dice\", \"cchvae\", \"cruds\", \"proto\", \"roar\", \"wachter\", \"gs\", \"acdc\"]\n",
    "date = str(datetime.date(datetime.now()))\n",
    "save_results_path = 'tables/appendix_carla_CI/{}/'.format(date)\n",
    "Path(save_results_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if dataset_name == 'breast_cancer' or dataset_name == 'give_me_some_credit':\n",
    "    metrics = ['trade_off', 'LOF',  'L1_distance', 'Constraint_Violation']\n",
    "    metrics_non_model = metrics.copy()\n",
    "else:\n",
    "    metrics = ['trade_off', 'LOF',  'L1_distance', 'avg_time']\n",
    "    metrics_non_model = metrics.copy()\n",
    "\n",
    "if model_type == 'forest':\n",
    "    rms = 'acdc_dice_gs_proto_roar_retrained'\n",
    "    rms_array = ['acdc', 'dice', 'gs', 'proto', 'roar']\n",
    "elif dataset_name == 'mnist':\n",
    "    rms = 'dice_proto_roar_wachter_gs_acdc_retrained'\n",
    "    rms_array = ['dice', 'proto', 'roar', 'wachter', 'gs', 'acdc']\n",
    "\n",
    "rms = 'crud_retrained'\n",
    "rms_array = ['crud']\n",
    "\n",
    "suffix = '_crud'\n",
    "\n",
    "\n",
    "metrics = [\n",
    "                        \"L0_distance\",\n",
    "                        \"L1_distance\",\n",
    "                        \"L2_distance\",\n",
    "                        \"Linf_distance\",\n",
    "                        \"Constraint_Violation\",\n",
    "                        \"Redundancy\",\n",
    "                        \"y-Nearest-Neighbours\",\n",
    "                        \"avg_time\"\n",
    "                    ]\n",
    "\n",
    "distance_metrics = [    \"L0_distance\",\n",
    "                        \"L1_distance\",\n",
    "                        \"L2_distance\",\n",
    "                        \"Linf_distance\",\n",
    "                        \"Redundancy\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "recourse_results = pd.read_csv(result_path + '/{}/{}/{}/results.csv'.format(dataset_name, model_type_dict, rms))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 89,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "result_table = recourse_results[recourse_results['ML_Model'] == model_type]\n",
    "\n",
    "result_table[distance_metrics] = result_table[distance_metrics]/num_features\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------crud_________L0_distance--------------\n",
      "1.0,1.0\n",
      "----------------crud_________L1_distance--------------\n",
      "ConfidenceInterval(low=0.2983116684457314, high=0.32447693842739916)\n",
      "----------------crud_________L2_distance--------------\n",
      "ConfidenceInterval(low=0.15254645471089573, high=0.1729537907087135)\n",
      "----------------crud_________Linf_distance--------------\n",
      "ConfidenceInterval(low=0.06879662856553051, high=0.07354017824550092)\n",
      "----------------crud_________Constraint_Violation--------------\n",
      "0.0,0.0\n",
      "----------------crud_________Redundancy--------------\n",
      "ConfidenceInterval(low=0.9190909090909087, high=0.9318181818181815)\n",
      "----------------crud_________y-Nearest-Neighbours--------------\n",
      "1.0,1.0\n",
      "----------------crud_________avg_time--------------\n",
      "2.265896280538291,2.265896280538291\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame(dtype=\"string\")\n",
    "for rm in rms_array:\n",
    "    result_rm = pd.DataFrame(dtype=\"string\")\n",
    "    result_rm['Recourse_Method'] = [rm]\n",
    "    for metric in metrics:\n",
    "        print(\"----------------{}_________{}--------------\".format(rm, metric))\n",
    "        data = result_table[result_table['Recourse_Method'] == rm][metric]\n",
    "        if data.min() == data.max():\n",
    "            low = data.min()\n",
    "            high = data.max()\n",
    "            print(\"{},{}\".format(data.mean(), data.mean()))\n",
    "        else:\n",
    "            data = (data, )\n",
    "            res = bootstrap(data, np.mean)\n",
    "            low = res.confidence_interval.low\n",
    "            high = res.confidence_interval.high\n",
    "            data = data[0]\n",
    "            print(res.confidence_interval)\n",
    "        result_rm[metric] = ['{:.2f} ({:.2f}, {:.2f})'.format(data.mean(), low, high )]\n",
    "\n",
    "    result = pd.concat([result, result_rm])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "success_result = pd.DataFrame()\n",
    "for rm in rms_array:\n",
    "    successful =int(result_table[result_table['Recourse_Method'] == rm]['Success_Rate'].mean() * 100)\n",
    "    result_rm = pd.DataFrame(dtype=\"string\")\n",
    "    result_rm['Recourse_Method'] = [rm]\n",
    "    if successful == 100:\n",
    "        result_rm['Success_Rate'] = ['{:.2f} ({:.2f}, {:.2f})'.format(1.0, 1.0, 1.0 )]\n",
    "    else:\n",
    "        success_table = np.array([1]*successful + [0] * (100-successful))\n",
    "        data = (success_table, )\n",
    "        res = bootstrap(data, np.mean)\n",
    "        low = res.confidence_interval.low\n",
    "        high = res.confidence_interval.high\n",
    "        data = data[0]\n",
    "        print(res.confidence_interval)\n",
    "        result_rm['Success_Rate'] = ['{:.2f} ({:.2f}, {:.2f})'.format(data.mean(), low, high )]\n",
    "    success_result = pd.concat([success_result, result_rm])\n",
    "\n",
    "result = result.set_index('Recourse_Method')\n",
    "success_result = success_result.set_index('Recourse_Method')\n",
    "result = result.join(success_result, on='Recourse_Method')\n",
    "#success_table = pd.DataFrame([1]*successful + [0] * (100-successful))\n",
    "#result_table[['Success_Rate', \"Recourse_Method\"]].groupby('Recourse_Method').mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "#result['Success_Rate'] = result_table[['Success_Rate', \"Recourse_Method\"]].groupby('Recourse_Method').mean()['Success_Rate']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "#order_of_columns = ['Success_Rate', \"Success_retrained_ann_models\", \"Success_retrained_linear_models\", \"Success_retrained_forest_models\", \"trade-off\", \"LOF\", \"L1_Distance\"]\n",
    "order_of_columns = ['Success_Rate' ] + metrics\n",
    "#result = result.join(result_table[['Success_Rate', \"Recourse_Method\"]].groupby('Recourse_Method').mean(), on='Recourse_Method').set_index('Recourse_Method')\n",
    "result = result.sort_index()\n",
    "result = result[order_of_columns]\n",
    "result.to_csv('{}{}_{}.csv'.format(save_results_path, dataset_name, model_type + suffix), float_format='%.2f')\n",
    "\n",
    "#small_result_table.to_csv('{}{}_{}_small.csv'.format(save_results_path, dataset_name, model_type), float_format='%.2f')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-6d0a8c1b",
   "language": "python",
   "display_name": "PyCharm (CARLA)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}